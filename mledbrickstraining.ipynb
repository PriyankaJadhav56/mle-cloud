{"cells":[{"cell_type":"markdown","source":["##What is bigdata?"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"1f99b16b-e727-4c7c-922e-b33261d0797c","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["Big data primarily refers to data sets that are too large or complex to be dealt with by traditional data-processing application software. Data with many entries offer greater statistical power, while data with higher complexity may lead to a higher false discovery rate."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"6d045779-2c8f-47af-93d7-a69b1a710a83","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["## Why spark?"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"00b2e975-ee16-4d4f-b98a-d1af00576c1e","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["when you think of a “computer” you think about one machine sitting on your desk at home or at work. This\nmachine works perfectly well for watching movies or working with spreadsheet software. However, as many users\nlikely experience at some point, there are some things that your computer is not powerful enough to perform. One\nparticularly challenging area is data processing. Single machines do not have enough power and resources to perform\ncomputations on huge amounts of information (or the user may not have time to wait for the computation to finish).\nA cluster, or group of machines, pools the resources of many machines together allowing us to use all the cumulative\nresources as if they were one. Now a group of machines alone is not powerful, you need a framework to coordinate\nwork across them. Spark is a tool for just that, managing and coordinating the execution of tasks on data across a\ncluster of computers."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"ce44b932-2802-445b-9719-3a192ec8055c","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["## What is spark?"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"bcdb22e9-43fc-41c8-835f-83c9ddc5db1c","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["Spark is a tool for managing and coordinating the execution of tasks on data across a cluster of computers.\nThe cluster of machines that Spark will leverage to execute tasks will be managed by a cluster manager like Spark’s\nStandalone cluster manager, YARN, or Mesos. We then submit Spark Applications to these cluster managers which will\ngrant resources to our application so that we can complete our work.\nSpark, in addition to its cluster mode, also has a local mode"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"3f7288fa-9e53-4fdf-b8b6-4238eaf47fe9","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["## Internals of spark?"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"303a20a3-1a58-4aa3-95d4-ccfc930e8155","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["Spark core\nSpark consists of one of the bigger components called Spark core which contains the majority of the libraries. On top of this Spark core, there are four different components. They are\nSpark SQL\nSpark Streaming\nMLLIB\nGraphX\n\nCluster Managers: \nSpark also consists of three pluggable cluster managers.\n\nStandalone: \nIt is a simple cluster manager which is easier to set up and execute the tasks on the cluster. It is a reliable cluster manager which can handle failures successfully. It can manage the resources based on the application requirements.\n\nApache Mesos: \nIt is a general cluster manager from the Apache group that can also run Hadoop MapReduce along with Spark and other service applications. It consists of API for most of the programming languages.\n \nHadoop YARN: \nIt is a resource manager which was provided in Hadoop 2. It stands for Yet Another Resource Negotiator. It is also a general-purpose cluster manager and can work in both Hadoop and Spark."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"da8087e6-fde0-4ec1-8e58-90ee34a33424","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["## Highlevel API of spark?"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"5ad2e8e5-3cad-4c29-9f6e-1ff27fb97066","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### SparkSQL and DataFrames.\n\nSparkSQL is the module in Spark for processing structured data also using DataFrames.\n\nDataFrame is a structured data collection formed of rows which is distributed across worker nodes (executer) of Spark. Fundamentally DataFrames are like tables in a relational database with their own schemas and headers."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"d7046ab5-00f7-42d9-bd14-6f02660bdf21","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### Sparksession:\n\nwe control our Spark Application through a driver process. This driver\nprocess manifests itself to the user as something called the SparkSession.\nSparkSession will be the entrance point to running Spark code.\nThe SparkSession instance is the way Spark exeutes user-defined manipulations across the cluster. \nIn Scala and Python the variable is available as spark when you start up the console.\nWhen using Spark from a Python or R, the user never writes explicit JVM instructions, but instead writes Python and R code that Spark will translate into code that Spark can then run on the executor JVMs."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"ebc412c7-dcb0-46e2-9eaa-08a53563b4fb","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### Dataframe:\n\nIn Spark, DataFrames are the distributed collections of data, organized into rows and columns. Each column in a DataFrame has a name and an associated type. DataFrames are similar to traditional database tables, which are structured and concise."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"a3291c41-4fc6-4238-8777-98e662a8fee4","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["Let’s now perform the simple task of creating a range of numbers. This range of numbers is just like a named column\nin a spreadsheet."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"841d5084-66bd-4dfc-b5d6-e4cd059304f8","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["myRange = spark.range(1000).toDF('number')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"f77d1f88-75dd-47f7-be09-85b5df4a7d71","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Partitions:\n    \n    In order to allow every executor to perform work in parallel, Spark breaks up the data into chunks, called partitions. A\npartition is a collection of rows that sit on one physical machine in our cluster. A DataFrame’s partitions represent how\nthe data is physically distributed across your cluster of machines during execution. If you have one partition, Spark\nwill only have a parallelism of one even if you have thousands of executors. If you have many partitions, but only one\nexecutor Spark will still only have a parallelism of one because there is only one computation resource."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"fe8fbf83-254c-4230-a58a-d46ab6ee99bb","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### Transformation:\n    \n    In Spark, the core data structures are immutable meaning they cannot be changed once created. This might seem like\na strange concept at first, if you cannot change it, how are you supposed to use it? In order to “change” a DataFrame\nyou will have to instruct Spark how you would like to modify the DataFrame you have into the one that you want.\nThese instructions are called transformations."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"0e8c9b19-86de-4821-9237-650380806f79","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["Let’s perform a simple transformation to find all even numbers in our currentDataFrame."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"795e70a0-3fe9-4b29-9498-8191a1d1e3f8","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["divisBy2 = myRange.where('number % 2 = 0')\ndivisBy2.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"61fc3cec-8f55-4530-a687-a851778c4a63","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+------+\n|number|\n+------+\n|     0|\n|     2|\n|     4|\n|     6|\n|     8|\n|    10|\n|    12|\n|    14|\n|    16|\n|    18|\n|    20|\n|    22|\n|    24|\n|    26|\n|    28|\n|    30|\n|    32|\n|    34|\n|    36|\n|    38|\n+------+\nonly showing top 20 rows\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["### Actions:\n    \n    Transformations allow us to build up our logical transformation plan. To trigger the computation, we run an action. An\naction instructs Spark to compute a result from a series of transformations. The simplest action is count which gives\nus the total number of records in the DataFrame.\nThe simplest action is count which gives us the total number of records in the DataFrame."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"a7e12408-da9d-4b42-89f8-b3598edec97b","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["divisBy2.count()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"9d197764-e322-4ad7-b586-c208a5b5a2e9","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[3]: 500"]}],"execution_count":0},{"cell_type":"markdown","source":["### Lazy evaluation\nLazy evaulation means that Spark will wait until the very last moment to execute your transformations. In Spark,\ninstead of modifying the data quickly, we build up a plan of transformations that we would like to apply to our source\ndata. Spark, by waiting until the last minute to execute the code, will compile this plan from your raw, DataFrame\ntransformations, to an efficient physical plan that will run as efficiently as possible across the cluster."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"76363256-ed00-41e7-b258-ed513d714ba3","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["## Dataframes and SQL:\n    \n    DataFrames and SQL, in Spark, are the exact same\nthing. You can express your business logic in either language and Spark will compile that logic down to an underlying\nplan (that we see in the explain plan) before actually executing your code.\n\nAny DataFrame can be made into a table or view with one simple method call."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"688e1fff-8f6c-4570-b8a2-d6bc504a1fb1","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["storage_account_name = \"dbrickstraining\"\nstorage_account_key = \"bFeEi4U9QHCmoVqQHIllW1q4bdbG5r+86uswAEqrffH9pnidh7yIM6irGLPDLEbZtxQ7ys6JXNAA+ASt41QDtg==\"\n\ncontainer = \"mleazuretrainingcontainer\"\nmount_pointt = f\"/mnt/{container}\""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"26673a72-7b70-40a5-9f08-eeac976ddc29","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["spark.conf.set(\"fs.azure.account.key.dbrickstraining.blob.core.windows.net\",storage_account_key)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"8f0ce881-338b-40ba-8c46-28a8f99951c0","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["dbutils.fs.ls(f\"wasbs://{container}@{storage_account_name}.blob.core.windows.net/\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"96560832-4bc8-456c-b09c-88246ea09d15","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[6]: [FileInfo(path='wasbs://mleazuretrainingcontainer@dbrickstraining.blob.core.windows.net/2015-summary.csv', name='2015-summary.csv', size=7337, modificationTime=1677839315000),\n FileInfo(path='wasbs://mleazuretrainingcontainer@dbrickstraining.blob.core.windows.net/2015-summary.json', name='2015-summary.json', size=21624, modificationTime=1678336030000)]"]}],"execution_count":0},{"cell_type":"code","source":["%fs ls wasbs://mleazuretrainingcontainer@dbrickstraining.blob.core.windows.net/"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"206e6cab-145a-488d-a2e0-c2d430ff9911","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["wasbs://mleazuretrainingcontainer@dbrickstraining.blob.core.windows.net/2015-summary.csv","2015-summary.csv",7337,1677839315000],["wasbs://mleazuretrainingcontainer@dbrickstraining.blob.core.windows.net/2015-summary.json","2015-summary.json",21624,1678336030000]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"path","type":"\"string\"","metadata":"{}"},{"name":"name","type":"\"string\"","metadata":"{}"},{"name":"size","type":"\"long\"","metadata":"{}"},{"name":"modificationTime","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>wasbs://mleazuretrainingcontainer@dbrickstraining.blob.core.windows.net/2015-summary.csv</td><td>2015-summary.csv</td><td>7337</td><td>1677839315000</td></tr><tr><td>wasbs://mleazuretrainingcontainer@dbrickstraining.blob.core.windows.net/2015-summary.json</td><td>2015-summary.json</td><td>21624</td><td>1678336030000</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["flightData2015 = spark\\\n.read\\\n.option('inferSchema', 'true')\\\n.option('header', 'true')\\\n.csv('wasbs://mleazuretrainingcontainer@dbrickstraining.blob.core.windows.net/2015-summary.csv')\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"ec49412f-cbc0-40e9-b0e2-350b5afe1a69","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["flightData2015.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"f3c2596f-897d-4bc6-94e6-6002dcdcea74","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+--------------------+-------------------+-----+\n|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+--------------------+-------------------+-----+\n|       United States|            Romania|   15|\n|       United States|            Croatia|    1|\n|       United States|            Ireland|  344|\n|               Egypt|      United States|   15|\n|       United States|              India|   62|\n|       United States|          Singapore|    1|\n|       United States|            Grenada|   62|\n|          Costa Rica|      United States|  588|\n|             Senegal|      United States|   40|\n|             Moldova|      United States|    1|\n|       United States|       Sint Maarten|  325|\n|       United States|   Marshall Islands|   39|\n|              Guyana|      United States|   64|\n|               Malta|      United States|    1|\n|            Anguilla|      United States|   41|\n|             Bolivia|      United States|   30|\n|       United States|           Paraguay|    6|\n|             Algeria|      United States|    4|\n|Turks and Caicos ...|      United States|  230|\n|       United States|          Gibraltar|    1|\n+--------------------+-------------------+-----+\nonly showing top 20 rows\n\n"]}],"execution_count":0},{"cell_type":"code","source":["flightData2015.createOrReplaceTempView('flight_data_2015')\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"49ac9499-4155-414c-b9cb-36a02a727113","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Now we can query our data in SQL. To execute a SQL query, we’ll use the spark.sql function"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"e2132f0e-c1a3-40a5-a3d9-3ab35de87d48","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["sqlWay = spark.sql(\"\"\"\nSELECT DEST_COUNTRY_NAME, count(1)\nFROM flight_data_2015\nGROUP BY DEST_COUNTRY_NAME\n\"\"\")\n\ndataFrameWay = flightData2015\\\n.groupBy('DEST_COUNTRY_NAME')\\\n.count()\nsqlWay.explain()\ndataFrameWay.explain()\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"a76cc3ef-32b5-4aea-9ce6-531931f73273","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- HashAggregate(keys=[DEST_COUNTRY_NAME#116], functions=[finalmerge_count(merge count#151L) AS count(1)#139L])\n   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#116, 200), ENSURE_REQUIREMENTS, [plan_id=144]\n      +- HashAggregate(keys=[DEST_COUNTRY_NAME#116], functions=[partial_count(1) AS count#151L])\n         +- FileScan csv [DEST_COUNTRY_NAME#116] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[wasbs://mleazuretrainingcontainer@dbrickstraining.blob.core.windows.ne..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n\n\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- HashAggregate(keys=[DEST_COUNTRY_NAME#116], functions=[finalmerge_count(merge count#153L) AS count(1)#146L])\n   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#116, 200), ENSURE_REQUIREMENTS, [plan_id=165]\n      +- HashAggregate(keys=[DEST_COUNTRY_NAME#116], functions=[partial_count(1) AS count#153L])\n         +- FileScan csv [DEST_COUNTRY_NAME#116] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[wasbs://mleazuretrainingcontainer@dbrickstraining.blob.core.windows.ne..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["## Datasets:\n\nWe can use Datasets only when we need or want to.\nFor instance, I’ll define my own object and manipulate it via arbitrary map and filter functions. Once we’ve\nperformed our manipulations, Spark can automatically turn it back into a DataFrame and we can manipulate it\nfurther using the hundreds of functions that Spark includes.\nThis makes it easy to drop down to lower level, type secure coding when necessary, and move higher up to SQL for more rapid analysis."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"b630e3fc-2ad2-43d0-92ea-9c8f3c6bd786","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["## Basic Structured Operation"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"f01a3abd-3954-434f-b3cb-9438a7319287","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["Spark is effectively a programming language of its own. Internally, Spark uses an engine called Catalyst that maintains\nits own type information through the planning and processing of work. This may seem like overkill, but it doing so,\nthis opens up a wide variety of execution optimizations that make significant differences. Spark types map directly\nto the different language APIs that Spark maintains and there exists a lookup table for each of these in each of Scala,\nJava, Python, SQL, and R. Even if we use Spark’s Structured APIs from Python or R, the majority of our manipulations\nwill operate strictly on Spark types, not Python types. For example, the below code does not perform addition in Scala\nor Python, it actually performs addition purely in Spark."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"c158e0cd-b661-46ef-8794-3e8c40acf768","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;36m  File \u001B[0;32m\"<command-1410129964961973>\"\u001B[0;36m, line \u001B[0;32m1\u001B[0m\n\u001B[0;31m    Spark is effectively a programming language of its own. Internally, Spark uses an engine called Catalyst that maintains\u001B[0m\n\u001B[0m                         ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n","errorSummary":"<span class='ansi-red-fg'>SyntaxError</span>: invalid syntax (<command-1410129964961973>, line 1)","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;36m  File \u001B[0;32m\"<command-1410129964961973>\"\u001B[0;36m, line \u001B[0;32m1\u001B[0m\n\u001B[0;31m    Spark is effectively a programming language of its own. Internally, Spark uses an engine called Catalyst that maintains\u001B[0m\n\u001B[0m                         ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n"]}}],"execution_count":0},{"cell_type":"code","source":["df = spark.range(500).toDF('number')\ndf.select(df['number'] + 10)\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"3d0d338f-814b-4e6e-89cc-eeab59dc707e","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+------+\n|number|\n+------+\n|     0|\n|     1|\n|     2|\n|     3|\n|     4|\n|     5|\n|     6|\n|     7|\n|     8|\n|     9|\n|    10|\n|    11|\n|    12|\n|    13|\n|    14|\n|    15|\n|    16|\n|    17|\n|    18|\n|    19|\n+------+\nonly showing top 20 rows\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["### 1. Schemas:\nA schema defines the column names and types of a DataFrame. Users can define schemas manually or users can\nread a schema from a data source (often called schema on read)."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"8cc321cb-0213-45bf-8d1a-0e015f4cd566","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%fs ls wasbs://mleazuretrainingcontainer@dbrickstraining.blob.core.windows.net/\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"9ae8d06c-c7bd-4ee4-813d-f5a7962d0d41","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["wasbs://mleazuretrainingcontainer@dbrickstraining.blob.core.windows.net/2015-summary.csv","2015-summary.csv",7337,1677839315000],["wasbs://mleazuretrainingcontainer@dbrickstraining.blob.core.windows.net/2015-summary.json","2015-summary.json",21624,1678336030000]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"path","type":"\"string\"","metadata":"{}"},{"name":"name","type":"\"string\"","metadata":"{}"},{"name":"size","type":"\"long\"","metadata":"{}"},{"name":"modificationTime","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>wasbs://mleazuretrainingcontainer@dbrickstraining.blob.core.windows.net/2015-summary.csv</td><td>2015-summary.csv</td><td>7337</td><td>1677839315000</td></tr><tr><td>wasbs://mleazuretrainingcontainer@dbrickstraining.blob.core.windows.net/2015-summary.json</td><td>2015-summary.json</td><td>21624</td><td>1678336030000</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["spark.read.format('json')\\\n.load('wasbs://mleazuretrainingcontainer@dbrickstraining.blob.core.windows.net/2015-summary.json')\\\n.schema"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"6ca8cdaf-13e2-4272-ae77-c26356062b3d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[14]: StructType([StructField('DEST_COUNTRY_NAME', StringType(), True), StructField('ORIGIN_COUNTRY_NAME', StringType(), True), StructField('count', LongType(), True)])"]}],"execution_count":0},{"cell_type":"markdown","source":["A schema is a StructType made up of a number of fields, StructFields, that have a name, type, and a boolean\nflag which specifies whether or not that column can contain missing or null values. Schemas can also contain other\nStructType (Spark’s complex types). We will see this in the next chapter when we discuss working with complex\ntypes.\nHere’s how to create, and enforce a specific schema on a DataFrame. If the types in the data (at runtime), do not\nmatch the schema. Spark will throw an error"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"49cceb13-f93c-4fbd-94d1-00122410cb89","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql.types import StructField, StructType, StringType, LongType\nmyManualSchema = StructType([\nStructField('DEST_COUNTRY_NAME', StringType(), True),\nStructField('ORIGIN_COUNTRY_NAME', StringType(), True),\nStructField('count', LongType(), False)\n])\ndf = spark.read.format('json')\\\n.schema(myManualSchema)\\\n.load('wasbs://mleazuretrainingcontainer@dbrickstraining.blob.core.windows.net/2015-summary.json')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"bf911ad2-8341-43ba-95d2-82b8b4c726b7","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 2. Columns and Expressions\nTo users, columns in Spark are similar to columns in a spreadsheet, R dataframe, pandas DataFrame. We can select,\nmanipulate, and remove columns from DataFrames and these operations are represented as expressions.\nTo Spark, columns are logical constructions that simply represent a value computed on a per-record basis by means\nof an expression. This means, in order to have a real value for a column, we need to have a row, and in order to\nhave a row we need to have a DataFrame. This means that we cannot manipulate an actual column outside of a\nDataFrame, we can only manipulate a logical column’s expressions then perform that expression within the context of\na DataFrame."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"abee4200-f512-4a9a-97f8-f607262c54f5","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### Columns:\nThere are a lot of different ways to construct and or refer to columns but the two simplest ways are with the col or\ncolumn functions. To use either of these functions, we pass in a column name."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"1dae2453-d8e0-4901-9cc2-c3e1ed022adb","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql.functions import col, column\ncol(\"someColumnName\")\ncolumn(\"someColumnName\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"4c8a8311-3034-4b15-b2aa-3682e94950a8","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[16]: Column<'someColumnName'>"]}],"execution_count":0},{"cell_type":"markdown","source":["### Expression:\nColumns provide a subset of expression functionality. If you use col() and wish to perform transformations on\nthat column, you must perform those on that column reference. When using an expression, the expr function can\nactually parse transformations and column references from a string and can subsequently be passed into further\ntransformations. Let’s look at some examples.\nexpr(“someCol - 5”) is the same transformation as performing col(“someCol”) - 5 or even\nexpr(“someCol”) - 5. That’s because Spark compiles these to a logical tree specifying the order of operations."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"79b38a69-0f79-4b43-82f9-dd26811943ca","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["(((col('someCol') + 5) * 200) - 6) < col('otherCol') "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"2ff96d57-07aa-4650-ad48-8167743cd573","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[17]: Column<'((((someCol + 5) * 200) - 6) < otherCol)'>"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import expr\nexpr('(((someCol + 5) * 200) - 6) < otherCol')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"d64dbea2-8b03-4ffe-bb3e-f0967b268b6a","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[18]: Column<'((((someCol + 5) * 200) - 6) < otherCol)'>"]}],"execution_count":0},{"cell_type":"markdown","source":["### 3. Records and Rows\nIn Spark, a record or row makes up a “row” in a DataFrame. A logical record or row is an object of type Row. Row\nobjects are the objects that column expressions operate on to produce some usable value. Row objects represent\nphysical byte arrays. The byte array interface is never shown to users because we only use column expressions to\nmanipulate them."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"375529d9-6b0e-4666-8a7d-37a55737ad0c","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df.first()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"c3bbf19e-890b-4e72-a5ae-1da6c062a125","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[19]: Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15)"]}],"execution_count":0},{"cell_type":"markdown","source":["### 4. Create Rows\nYou can create rows by manually instantiating a Row object with the values that below in each column. It’s important\nto note that only DataFrames have schema. Rows themselves do not have schemas. This means if you create a Row manually, you must specify the values in the same order as the schema of the DataFrame they may be appended to. \n\nA capitalized “Row” will refer to the Row object. We can see a row by calling first on our DataFrame."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"802ef3c1-b70f-4caa-8b15-01c67ff6a9d4","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql import Row\nmyRow = Row('Hello', None, 1, False)\nmyRow[0]"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"5d8b3edb-ab67-4322-91bb-92f0a39abc31","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[20]: 'Hello'"]}],"execution_count":0},{"cell_type":"code","source":["myRow[2]"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"3d100105-9c1b-4101-8230-e990aced797a","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[21]: 1"]}],"execution_count":0},{"cell_type":"markdown","source":["### 5. Dataframe Transformation\nNow that we briefly defined the core parts of a DataFrame, we will move onto manipulating DataFrames. When\nworking with individual DataFrames there are some fundamental objectives. These break down into several core\noperations.\n\n- We can add rows or columns\n- We can remove rows or columns\n- We can transform a row into a column (or vice versa)\n- We can change the order of rows based on the values in columns\n\nLuckily we can translate all of these into simple transformations, the most common being those that take one column,\nchange it row by row, and then return our results."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"add74788-b9b2-443a-8877-53c670cd7173","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["### 6. Creating Dataframes\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"6c2e9cbc-b307-41b9-8ea6-da7ad2ddd686","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# We can create DataFrames from raw data sources.\n\ndf = spark.read.format('json')\\\n.load('wasbs://mleazuretrainingcontainer@dbrickstraining.blob.core.windows.net/2015-summary.json')\ndf.createOrReplaceTempView('dfTable')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"0c04b00d-83b8-488b-8a1e-70f85d557724","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# We can also create DataFrames on the fly by taking a set of rows and converting them to a DataFrame.\n\nfrom pyspark.sql import Row\nfrom pyspark.sql.types import StructField, StructType,\\\n StringType, LongType\nmyManualSchema = StructType([\nStructField('some', StringType(), True),\nStructField('col', StringType(), True),\nStructField('names', LongType(), False)\n])\nmyRow = Row('Hello', None, 1)\nmyDf = spark.createDataFrame([myRow], myManualSchema)\nmyDf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"e26dfb21-c28c-4492-bae1-0a5af011205e","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----+----+-----+\n| some| col|names|\n+-----+----+-----+\n|Hello|null|    1|\n+-----+----+-----+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["### 7. select and selectExpr\nSelect and SelectExpr allow us to do the DataFrame equivalent of SQL queries on a table of data."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"149bebaa-916a-47d7-865a-512de1b278a4","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df.select(\n'DEST_COUNTRY_NAME',\n'ORIGIN_COUNTRY_NAME' )\\\n.show(2)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"f5d3d905-2417-4c7d-b62f-dddc0899c5df","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----------------+-------------------+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|\n+-----------------+-------------------+\n|    United States|            Romania|\n|    United States|            Croatia|\n+-----------------+-------------------+\nonly showing top 2 rows\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import expr, col, column\ndf.select(\nexpr('DEST_COUNTRY_NAME'),\ncol('DEST_COUNTRY_NAME'),\ncolumn('DEST_COUNTRY_NAME'))\\\n.show(2)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"a23547a7-5458-4f39-931a-21313482d188","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----------------+-----------------+-----------------+\n|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|\n+-----------------+-----------------+-----------------+\n|    United States|    United States|    United States|\n|    United States|    United States|    United States|\n+-----------------+-----------------+-----------------+\nonly showing top 2 rows\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["We can treat selectExpr as a simple way to build up complex expressions\nthat create new DataFrames. In fact, we can add any valid non-aggregating SQL statement and as long as the columns\nresolve — it will be valid! Here’s a simple example that adds a new column withinCountry to our DataFrame that\nspecifies whether or not the destination and origin are the same."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"6c4a68aa-d5c8-42a6-b6a3-c793e984eb4e","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df.selectExpr(\n'*', # all original columns\n'(DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry')\\\n.show(2)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"973fdc3e-2bf7-40ca-9812-4cd5383f292e","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----------------+-------------------+-----+-------------+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|\n+-----------------+-------------------+-----+-------------+\n|    United States|            Romania|   15|        false|\n|    United States|            Croatia|    1|        false|\n+-----------------+-------------------+-----+-------------+\nonly showing top 2 rows\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df.selectExpr('avg(count)', 'count(distinct(DEST_COUNTRY_NAME))').show(2)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"a609636e-ae9c-4db5-8e55-e5fa23019866","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----------+---------------------------------+\n| avg(count)|count(DISTINCT DEST_COUNTRY_NAME)|\n+-----------+---------------------------------+\n|1770.765625|                              132|\n+-----------+---------------------------------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["### 8. Adding columns\nThere’s also a more formal way of adding a new column to a DataFrame using the withColumn method on our\nDataFrame. For example, let’s add a column that just adds the number one as a column."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"7ce8cffd-4206-46c8-bcd5-27b3406540fb","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df.withColumn(\n'withinCountry',\nexpr('ORIGIN_COUNTRY_NAME == DEST_COUNTRY_NAME'))\\\n.show(2)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"bec20cfe-5b84-4388-ae95-c90a3f83e2fd","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----------------+-------------------+-----+-------------+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|\n+-----------------+-------------------+-----+-------------+\n|    United States|            Romania|   15|        false|\n|    United States|            Croatia|    1|        false|\n+-----------------+-------------------+-----+-------------+\nonly showing top 2 rows\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["### 9. Renaming columns\nwe can rename a column, it’s often much easier (and readable) to use the withColumnRenamed method. This will rename the column with the name of the string in the first argument, to the string in the second argument"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"807ba3bf-1f08-4867-8d83-7852384083bc","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df.withColumnRenamed('DEST_COUNTRY_NAME', 'dest').columns"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"3a3c6395-a45e-4e54-aeb8-5624e9c779ff","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[30]: ['dest', 'ORIGIN_COUNTRY_NAME', 'count']"]}],"execution_count":0},{"cell_type":"markdown","source":["### 10. Changing column's type\nSometimes we may need to convert from one type to another, for example if we have a set of StringType that\nshould be integers. We can convert columns from one type to another by casting the column from one type to another.\nFor instance let’s convert our count column from an integer to a Long type."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"cc590084-f76e-467a-abe3-3da24221fef0","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df.printSchema()\ndf.withColumn('count', col('count').cast('int')).printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"9f3fdd5f-5e3e-4a80-8365-8a3ea35eeb4f","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- DEST_COUNTRY_NAME: string (nullable = true)\n |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n |-- count: long (nullable = true)\n\nroot\n |-- DEST_COUNTRY_NAME: string (nullable = true)\n |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n |-- count: integer (nullable = true)\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["### 11. Filtering rows\nTo filter rows we create an expression that evaluates to true or false. We then filter out the rows that have expression\nthat is equal to false. The most common way to do this with DataFrames is to create either an expression as a String\nor build an expression with a set of column manipulations. There are two methods to perform this operation, we can\nuse where or filter and they both will perform the same operation and accept the same argument types when\nused with DataFrames."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"93cb8a20-d099-43a9-a1a0-192fb708937c","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["colCondition = df.filter(col('count') < 2).take(2)\nconditional = df.where('count < 2').take(2)\nconditional"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"f65147cd-f615-4fbc-b697-a6b07cefdb3f","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[34]: [Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Singapore', count=1)]"]}],"execution_count":0},{"cell_type":"code","source":["df.where(col('count') < 2)\\\n.where(col('ORIGIN_COUNTRY_NAME') != 'Croatia')\\\n.show(2)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"66cceaec-62d0-4591-a1fb-2d7ca6305526","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|          Singapore|    1|\n|          Moldova|      United States|    1|\n+-----------------+-------------------+-----+\nonly showing top 2 rows\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["### 12. Getting unique rows\nA very common use case is to get the unique or distinct values in a DataFrame. These values can be in one or more\ncolumns. The way we do this is with the distinct method on a DataFrame that will allow us to deduplicate\nany rows that are in that DataFrame. For instance let’s get the unique origins in our dataset"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"572b7733-fabe-4eeb-8076-7bbf05ce4439","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df.select('ORIGIN_COUNTRY_NAME', 'DEST_COUNTRY_NAME').count()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"00b67e4a-3b56-4c24-bc2a-d9b244ec12b0","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[36]: 256"]}],"execution_count":0},{"cell_type":"code","source":["df.select('ORIGIN_COUNTRY_NAME').distinct().count()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"fb2f80df-043c-4c96-8fd9-84d6ad1f149c","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[37]: 125"]}],"execution_count":0},{"cell_type":"markdown","source":["### 13. Sorting\nWhen we sort the values in a DataFrame, we always want to sort with either the largest or smallest values at the top of\na DataFrame. There are two equivalent operations to do this sort and orderBy that work the exact same way. They\naccept both column expressions and strings as well as multiple columns. The default is to sort in ascending order."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"59bdada2-6860-4693-a8bd-49ab9fd3782e","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df.sort('count').show(5)\ndf.orderBy('count', 'DEST_COUNTRY_NAME').show(5)\ndf.orderBy(col('count'), col('DEST_COUNTRY_NAME')).show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"a0f6aeba-978d-48aa-a5e5-dacd121f94db","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+--------------------+-------------------+-----+\n|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+--------------------+-------------------+-----+\n|               Malta|      United States|    1|\n|Saint Vincent and...|      United States|    1|\n|       United States|            Croatia|    1|\n|       United States|          Gibraltar|    1|\n|       United States|          Singapore|    1|\n+--------------------+-------------------+-----+\nonly showing top 5 rows\n\n+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|     Burkina Faso|      United States|    1|\n|    Cote d'Ivoire|      United States|    1|\n|           Cyprus|      United States|    1|\n|         Djibouti|      United States|    1|\n|        Indonesia|      United States|    1|\n+-----------------+-------------------+-----+\nonly showing top 5 rows\n\n+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|     Burkina Faso|      United States|    1|\n|    Cote d'Ivoire|      United States|    1|\n|           Cyprus|      United States|    1|\n|         Djibouti|      United States|    1|\n|        Indonesia|      United States|    1|\n+-----------------+-------------------+-----+\nonly showing top 5 rows\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import desc, asc\ndf.orderBy(expr('count desc')).show(2)\ndf.orderBy(desc(col('count')), asc(col('DEST_COUNTRY_NAME'))).show(2)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"2b932684-5478-4a3e-97df-2ad9c925bea1","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|          Moldova|      United States|    1|\n|    United States|            Croatia|    1|\n+-----------------+-------------------+-----+\nonly showing top 2 rows\n\n+-----------------+-------------------+------+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|\n+-----------------+-------------------+------+\n|    United States|      United States|370002|\n|    United States|             Canada|  8483|\n+-----------------+-------------------+------+\nonly showing top 2 rows\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["### 14. Limit\nOften times you may just want the top ten of some DataFrame. For example, you might want to only work with the top\n50 of some dataset. We do this with the limit method"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"406bac07-964c-4168-baef-b3508d5efb28","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df.limit(5).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"980ac60b-c9e9-4102-a639-be3ab95d35c2","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Romania|   15|\n|    United States|            Croatia|    1|\n|    United States|            Ireland|  344|\n|            Egypt|      United States|   15|\n|    United States|              India|   62|\n+-----------------+-------------------+-----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df.orderBy(expr('count desc')).limit(6).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"17fcf2c7-3a07-4249-b9bd-ca68476456e2","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+--------------------+-------------------+-----+\n|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+--------------------+-------------------+-----+\n|               Malta|      United States|    1|\n|Saint Vincent and...|      United States|    1|\n|       United States|            Croatia|    1|\n|       United States|          Gibraltar|    1|\n|       United States|          Singapore|    1|\n|             Moldova|      United States|    1|\n+--------------------+-------------------+-----+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["### 15. Repartition and Coalesce\nAnother important optimization opportunity is to partition the data according to some frequently filtered columns\nwhich controls the physical layout of data across the cluster including the partitioning scheme and the number of\npartitions"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"9a31f432-e77b-434f-82ff-272238411152","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df.rdd.getNumPartitions()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"58270e56-0b3d-4be4-a162-5b8c4f2c85a5","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[42]: 1"]}],"execution_count":0},{"cell_type":"code","source":["df.repartition(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"347a2d4c-fe38-4497-9284-9762354217cb","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[43]: DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint]"]}],"execution_count":0},{"cell_type":"markdown","source":["## Aggregation Functions\nAggregating is the act of collecting something together and is a cornerstone of big data analytics.\nIn an aggregation, you will specify a key or grouping and an aggregation function that specifies\nhow you should transform one or more columns. This function must produce one result for each\ngroup, given multiple input values. Spark’s aggregation capabilities are sophisticated and mature,\nwith a variety of different use cases and possibilities."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a4663c84-8c93-4471-9e27-a0ab433dfd38","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%fs ls wasbs://mleazuretrainingcontainer@dbrickstraining.blob.core.windows.net/"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"7283315d-ad09-40d7-a843-f59d7a71c53b","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["wasbs://mleazuretrainingcontainer@dbrickstraining.blob.core.windows.net/2015-summary.csv","2015-summary.csv",7337,1677839315000],["wasbs://mleazuretrainingcontainer@dbrickstraining.blob.core.windows.net/2015-summary.json","2015-summary.json",21624,1678336030000],["wasbs://mleazuretrainingcontainer@dbrickstraining.blob.core.windows.net/all/","all/",0,0]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"path","type":"\"string\"","metadata":"{}"},{"name":"name","type":"\"string\"","metadata":"{}"},{"name":"size","type":"\"long\"","metadata":"{}"},{"name":"modificationTime","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>wasbs://mleazuretrainingcontainer@dbrickstraining.blob.core.windows.net/2015-summary.csv</td><td>2015-summary.csv</td><td>7337</td><td>1677839315000</td></tr><tr><td>wasbs://mleazuretrainingcontainer@dbrickstraining.blob.core.windows.net/2015-summary.json</td><td>2015-summary.json</td><td>21624</td><td>1678336030000</td></tr><tr><td>wasbs://mleazuretrainingcontainer@dbrickstraining.blob.core.windows.net/all/</td><td>all/</td><td>0</td><td>0</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["df = spark.read.format(\"csv\")\\\n.option(\"header\", \"true\")\\\n.option(\"inferSchema\", \"true\")\\\n.load(\"wasbs://mleazuretrainingcontainer@dbrickstraining.blob.core.windows.net/all/*.csv\")\\\n.coalesce(5)\ndf.cache()\ndf.createOrReplaceTempView(\"dfTable\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"45a0a147-7e99-44b7-83d7-a4840e7e8c7a","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"1b7d244f-aa60-4f95-83af-2d14806db89b","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n|InvoiceNo|StockCode|         Description|Quantity|   InvoiceDate|UnitPrice|CustomerID|       Country|\n+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/2010 8:26|     2.55|     17850|United Kingdom|\n|   536365|    71053| WHITE METAL LANTERN|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n|   536365|   84406B|CREAM CUPID HEART...|       8|12/1/2010 8:26|     2.75|     17850|United Kingdom|\n|   536365|   84029G|KNITTED UNION FLA...|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n|   536365|   84029E|RED WOOLLY HOTTIE...|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n|   536365|    22752|SET 7 BABUSHKA NE...|       2|12/1/2010 8:26|     7.65|     17850|United Kingdom|\n|   536365|    21730|GLASS STAR FROSTE...|       6|12/1/2010 8:26|     4.25|     17850|United Kingdom|\n|   536366|    22633|HAND WARMER UNION...|       6|12/1/2010 8:28|     1.85|     17850|United Kingdom|\n|   536366|    22632|HAND WARMER RED P...|       6|12/1/2010 8:28|     1.85|     17850|United Kingdom|\n|   536367|    84879|ASSORTED COLOUR B...|      32|12/1/2010 8:34|     1.69|     13047|United Kingdom|\n|   536367|    22745|POPPY'S PLAYHOUSE...|       6|12/1/2010 8:34|      2.1|     13047|United Kingdom|\n|   536367|    22748|POPPY'S PLAYHOUSE...|       6|12/1/2010 8:34|      2.1|     13047|United Kingdom|\n|   536367|    22749|FELTCRAFT PRINCES...|       8|12/1/2010 8:34|     3.75|     13047|United Kingdom|\n|   536367|    22310|IVORY KNITTED MUG...|       6|12/1/2010 8:34|     1.65|     13047|United Kingdom|\n|   536367|    84969|BOX OF 6 ASSORTED...|       6|12/1/2010 8:34|     4.25|     13047|United Kingdom|\n|   536367|    22623|BOX OF VINTAGE JI...|       3|12/1/2010 8:34|     4.95|     13047|United Kingdom|\n|   536367|    22622|BOX OF VINTAGE AL...|       2|12/1/2010 8:34|     9.95|     13047|United Kingdom|\n|   536367|    21754|HOME BUILDING BLO...|       3|12/1/2010 8:34|     5.95|     13047|United Kingdom|\n|   536367|    21755|LOVE BUILDING BLO...|       3|12/1/2010 8:34|     5.95|     13047|United Kingdom|\n|   536367|    21777|RECIPE BOX WITH M...|       4|12/1/2010 8:34|     7.95|     13047|United Kingdom|\n+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\nonly showing top 20 rows\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["### 1. count\nwe can do one of two things: specify a specific\ncolumn to count, or all the columns by using count(*) or count(1) to represent that we want to\ncount every row as the literal one, as shown in this example:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1b496a43-0213-4dfa-9b48-9d3acb093e46","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql.functions import count\ndf.select(count(\"StockCode\")).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"127c8f59-d64d-47c8-b110-e1685bfa7b9e","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----------------+\n|count(StockCode)|\n+----------------+\n|          541909|\n+----------------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["### 2. countDistinct\nSometimes, the total number is not relevant; rather, it’s the number of unique groups that you\nwant. To get this number, you can use the countDistinct function. This is a bit more relevant\nfor individual columns:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"aa1431db-d0bc-4e25-90bb-7400ff9c2d36","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql.functions import countDistinct\ndf.select(countDistinct(\"StockCode\")).show() "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"660c8b41-6a79-4a54-bb36-ae28dc48557c","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------------------------+\n|count(DISTINCT StockCode)|\n+-------------------------+\n|                     4070|\n+-------------------------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["### 3. first and last\nYou can get the first and last values from a DataFrame by using these two obviously named\nfunctions. This will be based on the rows in the DataFrame, not on the values in the DataFrame:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e1502014-5fe4-4d02-ae69-e9a267fe9b64","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql.functions import first, last\ndf.select(first(\"StockCode\"), last(\"StockCode\")).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"f655584b-bf4d-40e3-a01e-87e89d485884","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----------------+---------------+\n|first(StockCode)|last(StockCode)|\n+----------------+---------------+\n|           23182|          23510|\n+----------------+---------------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["### 4. min and max\nTo extract the minimum and maximum values from a DataFrame, use the min and max functions:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"76d48618-a919-4e08-87dc-1e678cf8b68b","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql.functions import min, max\ndf.select(min(\"Quantity\"), max(\"Quantity\")).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"ca8fc4e1-5891-4fc5-a12c-71a67dee2ecc","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------------+-------------+\n|min(Quantity)|max(Quantity)|\n+-------------+-------------+\n|       -80995|        80995|\n+-------------+-------------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["### 5. sum\nAnother simple task is to add all the values in a row using the sum function:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9ebfda75-7e10-4382-a109-b5803d25da33","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql.functions import sum\ndf.select(sum(\"Quantity\")).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"1fe0f89d-9d8a-4d3c-8d0f-dec913553ec4","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------------+\n|sum(Quantity)|\n+-------------+\n|      5176450|\n+-------------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["### 6. sum\n Another simple task is to add all the values in a row using the sum function:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f94b3162-6781-4718-85c1-81f2c13cde72","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql.functions import sum\ndf.select(sum(\"Quantity\")).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"475c82e2-c623-4c19-adf5-985846adb470","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------------+\n|sum(Quantity)|\n+-------------+\n|      5176450|\n+-------------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["### 7. sumDistinct\nIn addition to summing a total, you also can sum a distinct set of values by using the\nsumDistinct function:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6b26a493-6961-4c83-8ae9-01c7c42ddac2","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql.functions import sumDistinct\ndf.select(sumDistinct(\"Quantity\")).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"b65fbadb-7e01-4f1f-9450-479c713b6f22","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["/databricks/spark/python/pyspark/sql/functions.py:386: FutureWarning: Deprecated in 3.2, use sum_distinct instead.\n  warnings.warn(\"Deprecated in 3.2, use sum_distinct instead.\", FutureWarning)\n+----------------------+\n|sum(DISTINCT Quantity)|\n+----------------------+\n|                 29310|\n+----------------------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["### 8. avg\nAlthough you can calculate average by dividing sum by count, Spark provides an easier way to\nget that value via the avg or mean functions. In this example, we use alias in order to more\neasily reuse these columns later:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4d592d36-a956-4d4b-b421-0940334bbdd9","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql.functions import sum, count, avg, expr\ndf.select(\ncount(\"Quantity\").alias(\"total_transactions\"),\nsum(\"Quantity\").alias(\"total_purchases\"),\navg(\"Quantity\").alias(\"avg_purchases\"),\nexpr(\"mean(Quantity)\").alias(\"mean_purchases\"))\\\n.selectExpr(\n\"total_purchases/total_transactions\",\n\"avg_purchases\",\n\"mean_purchases\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"ca5e108f-35bc-40af-aeb5-89e524a60e61","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+--------------------------------------+----------------+----------------+\n|(total_purchases / total_transactions)|   avg_purchases|  mean_purchases|\n+--------------------------------------+----------------+----------------+\n|                      9.55224954743324|9.55224954743324|9.55224954743324|\n+--------------------------------------+----------------+----------------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["### 9. grouping\n A more common task is to\nperform calculations based on groups in the data. This is typically done on categorical data for\nwhich we group our data on one column and perform some calculations on the other columns\nthat end up in that group."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6423f38e-53a0-49ed-ba31-14a875776931","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df.groupBy(\"InvoiceNo\", \"CustomerId\").count().show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"09f45df7-d45f-4696-a114-94a9265cb077","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---------+----------+-----+\n|InvoiceNo|CustomerId|count|\n+---------+----------+-----+\n|   536846|     14573|   76|\n|   537026|     12395|   12|\n|   537883|     14437|    5|\n|   538068|     17978|   12|\n|   538279|     14952|    7|\n|   538800|     16458|   10|\n|   538942|     17346|   12|\n|  C539947|     13854|    1|\n|   540096|     13253|   16|\n|   540530|     14755|   27|\n|   541225|     14099|   19|\n|   541978|     13551|    4|\n|   542093|     17677|   16|\n|   536596|      null|    6|\n|   537252|      null|    1|\n|   538041|      null|    1|\n|   537159|     14527|   28|\n|   537213|     12748|    6|\n|   538191|     15061|   16|\n|  C539301|     13496|    1|\n+---------+----------+-----+\nonly showing top 20 rows\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["### 10. window functions\nSpark supports three kinds of window functions: ranking functions, analytic functions,\nand aggregate functions"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5dc9ad89-d19e-4a1d-bbe7-d710bf24ba1e","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["## Joins\nA join brings together two sets of data, the left and the right, by comparing the value of one or\nmore keys of the left and right and evaluating the result of a join expression that determines\nwhether Spark should bring together the left set of data with the right set of data."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1251b0e3-356c-4bf9-8f5b-60ce355ccfcc","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### 1. Try out examples for each types of joins"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"809f7ad5-0f2a-466e-9a45-093730054edf","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["person = spark.createDataFrame([\n(0, \"Bill Chambers\", 0, [100]),\n(1, \"Matei Zaharia\", 1, [500, 250, 100]),\n(2, \"Michael Armbrust\", 1, [250, 100])])\\\n.toDF(\"id\", \"name\", \"graduate_program\", \"spark_status\")\ngraduateProgram = spark.createDataFrame([\n(0, \"Masters\", \"School of Information\", \"UC Berkeley\"),\n(2, \"Masters\", \"EECS\", \"UC Berkeley\"),\n(1, \"Ph.D.\", \"EECS\", \"UC Berkeley\")])\\\n.toDF(\"id\", \"degree\", \"department\", \"school\")\nsparkStatus = spark.createDataFrame([\n(500, \"Vice President\"),\n(250, \"PMC Member\"),\n(100, \"Contributor\")])\\\n.toDF(\"id\", \"status\")\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"6fb603a2-d51c-44a6-acd7-cd2dc18b309c","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["person.createOrReplaceTempView(\"person\")\ngraduateProgram.createOrReplaceTempView(\"graduateProgram\")\nsparkStatus.createOrReplaceTempView(\"sparkStatus\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"330b7801-30a0-4d0e-ad6a-565d3304c8d0","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Inner Joins\nInner joins evaluate the keys in both of the DataFrames or tables and include (and join together)\nonly the rows that evaluate to true. In the following example, we join the graduateProgram\nDataFrame with the person DataFrame to create a new DataFrame:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ad25efae-26a4-4a69-b271-9c20d350d13a","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["joinExpression = person[\"graduate_program\"] == graduateProgram['id']"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"e32c3eea-7ec4-44da-9be9-0d52ccbe4e57","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Outer Joins\nOuter joins evaluate the keys in both of the DataFrames or tables and includes (and joins\ntogether) the rows that evaluate to true or false. If there is no equivalent row in either the left or\nright DataFrame, Spark will insert null"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f0545680-d78f-4220-8e7f-1ca88de3eae5","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["joinType = \"outer\"\nperson.join(graduateProgram, joinExpression, joinType).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"b4852b58-8317-477c-a894-1c00b417e2ab","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n|  id|            name|graduate_program|   spark_status| id| degree|          department|     school|\n+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n|   0|   Bill Chambers|               0|          [100]|  0|Masters|School of Informa...|UC Berkeley|\n|   1|   Matei Zaharia|               1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n|   2|Michael Armbrust|               1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n|null|            null|            null|           null|  2|Masters|                EECS|UC Berkeley|\n+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["#### Left Outer Joins\nLeft outer joins evaluate the keys in both of the DataFrames or tables and includes all rows from\nthe left DataFrame as well as any rows in the right DataFrame that have a match in the left\nDataFrame. If there is no equivalent row in the right DataFrame, Spark will insert null:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7f2dc074-f417-490e-9819-85e29dc598e4","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["joinType = \"left_outer\"\ngraduateProgram.join(person, joinExpression, joinType).show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"2fdca992-4b0f-47c2-9a9e-c46bd7ba6f59","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+-------+--------------------+-----------+----+----------------+----------------+---------------+\n| id| degree|          department|     school|  id|            name|graduate_program|   spark_status|\n+---+-------+--------------------+-----------+----+----------------+----------------+---------------+\n|  0|Masters|School of Informa...|UC Berkeley|   0|   Bill Chambers|               0|          [100]|\n|  2|Masters|                EECS|UC Berkeley|null|            null|            null|           null|\n|  1|  Ph.D.|                EECS|UC Berkeley|   2|Michael Armbrust|               1|     [250, 100]|\n|  1|  Ph.D.|                EECS|UC Berkeley|   1|   Matei Zaharia|               1|[500, 250, 100]|\n+---+-------+--------------------+-----------+----+----------------+----------------+---------------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["#### Right Outer Joins\nRight outer joins evaluate the keys in both of the DataFrames or tables and includes all rows\nfrom the right DataFrame as well as any rows in the left DataFrame that have a match in the right\nDataFrame. If there is no equivalent row in the left DataFrame, Spark will insert null:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1f69b818-754d-429c-a389-6c0e5033c82f","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["joinType = \"right_outer\"\nperson.join(graduateProgram, joinExpression, joinType).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"dfe58a22-a222-4259-9c61-9e278f09955d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n|  id|            name|graduate_program|   spark_status| id| degree|          department|     school|\n+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n|   0|   Bill Chambers|               0|          [100]|  0|Masters|School of Informa...|UC Berkeley|\n|null|            null|            null|           null|  2|Masters|                EECS|UC Berkeley|\n|   2|Michael Armbrust|               1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n|   1|   Matei Zaharia|               1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["#### Left Semi Joins\nSemi joins are a bit of a departure from the other joins. They do not actually include any values\nfrom the right DataFrame. They only compare values to see if the value exists in the second\nDataFrame. If the value does exist, those rows will be kept in the result, even if there are\nduplicate keys in the left DataFrame"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b6da31ab-37d0-4f01-ab5a-12234c887f21","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["joinType = \"left_semi\"\ngraduateProgram.join(person, joinExpression, joinType).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"b2fa28e8-ebfe-4ac3-8012-2022e173afa7","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+-------+--------------------+-----------+\n| id| degree|          department|     school|\n+---+-------+--------------------+-----------+\n|  0|Masters|School of Informa...|UC Berkeley|\n|  1|  Ph.D.|                EECS|UC Berkeley|\n+---+-------+--------------------+-----------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["#### Left Anti Joins\n\nLeft anti joins are the opposite of left semi joins. Like left semi joins, they do not actually\ninclude any values from the right DataFrame. They only compare values to see if the value exists\nin the second DataFrame."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5f5d4cdf-a8a6-462c-9973-692d7190017a","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["joinType = \"left_anti\"\ngraduateProgram.join(person, joinExpression, joinType).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"90fa0644-698c-4bd3-bbd8-4e18f8d2d7c4","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+-------+----------+-----------+\n| id| degree|department|     school|\n+---+-------+----------+-----------+\n|  2|Masters|      EECS|UC Berkeley|\n+---+-------+----------+-----------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["#### Natural Joins\nNatural joins make implicit guesses at the columns on which you would like to join. It finds\nmatching columns and returns the results. Left, right, and outer natural joins are all supported."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8060d0ba-ae0d-49b9-9386-65eef722c45b","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["#### Cross (Cartesian) Joins\nThe last of our joins are cross-joins or cartesian products. Cross-joins in simplest terms are inner\njoins that do not specify a predicate. Cross joins will join every single row in the left DataFrame\nto ever single row in the right DataFrame."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"49834b17-fa0d-4655-b686-17343db3a3b7","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["joinType = \"cross\"\ngraduateProgram.join(person, joinExpression, joinType).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"97b33a72-ef1a-4222-9a48-537ff3518eca","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+-------+--------------------+-----------+---+----------------+----------------+---------------+\n| id| degree|          department|     school| id|            name|graduate_program|   spark_status|\n+---+-------+--------------------+-----------+---+----------------+----------------+---------------+\n|  0|Masters|School of Informa...|UC Berkeley|  0|   Bill Chambers|               0|          [100]|\n|  1|  Ph.D.|                EECS|UC Berkeley|  2|Michael Armbrust|               1|     [250, 100]|\n|  1|  Ph.D.|                EECS|UC Berkeley|  1|   Matei Zaharia|               1|[500, 250, 100]|\n+---+-------+--------------------+-----------+---+----------------+----------------+---------------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["### 2. Handling Duplicate column names\n\nOne of the tricky things that come up in joins is dealing with duplicate column names in your\nresults DataFrame. In a DataFrame, each column has a unique ID within Spark’s SQL Engine,\nCatalyst. This unique ID is purely internal and not something that you can directly reference.\nThis makes it quite difficult to refer to a specific column when you have a DataFrame with\nduplicate column names.\nThis can occur in two distinct situations:\nThe join expression that you specify does not remove one key from one of the input\nDataFrames and the keys have the same column name\nTwo columns on which you are not performing the join have the same name"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d792dcfd-2c3e-47dd-a4a8-4f7165179023","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["##### Approach 1: Different join expression\nWhen you have two keys that have the same name, probably the easiest fix is to change the join\nexpression from a Boolean expression to a string or sequence. This automatically removes one of\nthe columns for you during the join:\nperson.join(gradProgramDupe,\"graduate_program\").select(\"graduate_program\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"14e5ca95-a26f-41d1-9e89-56da2829f4c5","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["##### Approach 2: Dropping the column after the join\nAnother approach is to drop the offending column after the join. When doing this, we need to\nrefer to the column via the original source DataFrame. We can do this if the join uses the same\nkey names or if the source DataFrames have columns that simply have the same name:\nperson.join(gradProgramDupe, joinExpr).drop(person.col(\"graduate_program\"))\n.select(\"graduate_program\").show()\nval joinExpr = person.col(\"graduate_program\") === graduateProgram.col(\"id\")\nperson.join(graduateProgram, joinExpr).drop(graduateProgram.col(\"id\")).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"61664b32-f657-41f3-af3b-ac08088cab15","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["##### Approach 3: Renaming a column before the join\nWe can avoid this issue altogether if we rename one of our columns before the join:\nval gradProgram3 = graduateProgram.withColumnRenamed(\"id\", \"grad_id\")\nval joinExpr = person.col(\"graduate_program\") === gradProgram3.col(\"grad_id\")\nperson.join(gradProgram3, joinExpr).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b5fc1be2-4bde-41a4-8025-ff57bdd5bd16","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### 3. How spark performs joins\nTo understand how Spark performs joins, you need to understand the two core resources at play:\nthe node-to-node communication strategy and per node computation strategy. These internals are\nlikely irrelevant to your business problem. However, comprehending how Spark performs joins\ncan mean the difference between a job that completes quickly and one that never completes at\nall."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"208b3ff6-eea0-4915-a9ff-d7453f23e714","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["## Datasources\ny. Spark has six\n“core” data sources and hundreds of external data sources written by the community. The ability\nto read and write from all different kinds of data sources and for the community to create its own\ncontributions is arguably one of Spark’s greatest strengths. Following are Spark’s core data\nsources:\n- CSV\n- JSON\n- Parquet\n- ORC\n- JDBC/ODBC connections\n- Plain-text files"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a28bea42-1b86-422c-8ff1-667a27d2f5b3","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### 1. Basics of reading data"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f946b045-d62c-4212-8e85-8235382cc554","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### 2. Basics of write data\n\nThe foundation for reading data in Spark is the DataFrameReader. We access this through the\nSparkSession via the read attribute:\nspark.read\nAfter we have a DataFrame reader, we specify several values:\n- The format\n- The schema\n- The read mode\n- A series of options"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6500d564-e879-4c7b-95a1-cc06a9b60d5a","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### 3. CSV files - reading, writing\nTo read a CSV file, like any other format, we must first create a DataFrameReader for that\nspecific format. Here, we specify the format to be CSV:\nspark.read.format(\"csv\")\n\nJust as with reading data, there are a variety of options (listed in Table 9-3) for writing data when\nwe write CSV files. This is a subset of the reading options because many do not apply when\nwriting data (like maxColumns and inferSchema)."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"aff12acc-9651-492f-bd35-4ec2f00ac7a5","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### 4. REading and writing json files\n \n  There are some catches when working with this kind of data\nthat are worth considering before we jump in. In Spark, when we refer to JSON files, we refer to\nline-delimited JSON files. This contrasts with files that have a large JSON object or array per\nfile.\nThe line-delimited versus multiline trade-off is controlled by a single option: multiLine.\n\nWriting JSON files is just as simple as reading them, and, as you might expect, the data source\ndoes not matter. Therefore, we can reuse the CSV DataFrame that we created earlier to be the\nsource for our JSON file."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fccf2e72-6c61-477a-9fc8-8852af9c471b","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### 5. Parquet files - important"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b19a7c7d-90b7-41c6-b640-2b5b2b4f6799","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["Parquet is an open source column-oriented data store that provides a variety of storage\noptimizations, especially for analytics workloads. It provides columnar compression, which\nsaves storage space and allows for reading individual columns instead of entire files. It is a file\nformat that works exceptionally well with Apache Spark and is in fact the default file format."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"570f84d4-c935-4cab-a514-aa33b2a0dc10","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### 6. Reading and Writing parquet files\n\nParquet has very few options because it enforces its own schema when storing data. Thus, all you\nneed to set is the format and you are good to go. We can set the schema if we have strict\nrequirements for what our DataFrame should look like\nspark.read.format(\"parquet\")\n \nWriting Parquet is as easy as reading it. We simply specify the location for the file."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"00d1511f-2c0c-4b9c-af17-c3a8222a30db","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### 7. orc - optional"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0daede1e-cf3b-4301-b02e-c0f26ee5a979","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["ORC is a self-describing, type-aware columnar file format designed for Hadoop workloads. It is\noptimized for large streaming reads, but with integrated support for finding required rows\nquickly. ORC actually has no options for reading in data because Spark understands the file\nformat quite well."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"a6f26e68-672e-4d0f-9b20-e599de7aa640","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### 8. Splittable File Types and COmpression\n\nCertain file formats are fundamentally “splittable.” This can improve speed because it makes it\npossible for Spark to avoid reading an entire file, and access only the parts of the file necessary\nto satisfy your query. Additionally if you’re using something like Hadoop Distributed File\nSystem (HDFS), splitting a file can provide further optimization if that file spans multiple\nblocks. In conjunction with this is a need to manage compression. Not all compression schemes\nare splittable. How you store your data is of immense consequence when it comes to making\nyour Spark jobs run smoothly. We recommend Parquet with gzip compression."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"472428d9-2a89-42eb-915d-70790be991fa","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### 9. Managing File size\n\nManaging file sizes is an important factor not so much for writing data but reading it later on.\nWhen you’re writing lots of small files, there’s a significant metadata overhead that you incur\nmanaging all of those files. Spark especially does not do well with small files, although many file\nsystems (like HDFS) don’t handle lots of small files well, either. You might hear this referred to\nas the “small file problem.” The opposite is also true: you don’t want files that are too large\neither, because it becomes inefficient to have to read entire blocks of data when you need only a\nfew rows."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7792b615-6c86-4019-9322-4af99a0cbe1d","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["## Spark SQL \n\nSpark SQL is arguably one of the most important and powerful features in Spark."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"205cf652-db4b-4ac7-8bab-7403e7fa2489","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### What Is SQL?\nSQL or Structured Query Language is a domain-specific language for expressing relational\noperations over data"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"91fcdcfe-85d1-400c-925b-a4ce0d15ff15","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### Big Data and SQL: Apache Hive\nBefore Spark’s rise, Hive was the de facto big data SQL access layer. Originally developed at\nFacebook, Hive became an incredibly popular tool across industry for performing SQL\noperations on big data."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"df24626c-2500-4a69-9587-575b9202e415","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### Big Data and SQL: Spark SQL\nWith the release of Spark 2.0, its authors created a superset of Hive’s support, writing a native\nSQL parser that supports both ANSI-SQL as well as HiveQL queries."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5125e51b-e5e8-4f43-aa56-2be18c220baf","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### Spark’s Relationship to Hive\nSpark SQL has a great relationship with Hive because it can connect to Hive metastores. The\nHive metastore is the way in which Hive maintains table information for use across sessions."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"40a470c5-91b0-48bb-bc85-b17cbcbfe5ab","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### Catalog\nThe highest level abstraction in Spark SQL is the Catalog. The Catalog is an abstraction for the\nstorage of metadata about the data stored in your tables as well as other helpful things like\ndatabases, tables, functions, and views. The catalog is available in the\norg.apache.spark.sql.catalog.Catalog package and contains a number of helpful functions\nfor doing things like listing tables, databases, and functions."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b1945136-6824-4598-88c4-b7adbf09b9b4","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### Tables\nTo do anything useful with Spark SQL, you first need to define tables. Tables are logically\nequivalent to a DataFrame in that they are a structure of data against which you run commands.\nWe can join tables, filter them, aggregate them, and perform different manipulations."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"33ef9e0b-df62-4136-ba31-cbe0e3ccb124","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### Spark-Managed Tables\nOne important note is the concept of managed versus unmanaged tables. Tables store two\nimportant pieces of information. The data within the tables as well as the data about the tables;\nthat is, the metadata."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"418e40a2-d85f-4f8c-a343-51893aec9744","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### Creating Tables\nYou can create tables from a variety of sources. Something fairly unique to Spark is the\ncapability of reusing the entire Data Source API within SQL. This means that you do not need to\ndefine a table and then load data into it; Spark lets you create one on the fly. You can even\nspecify all sorts of sophisticated options when you read in a file. For example,"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e70dcb6e-97e4-4dc0-919a-989ce71533c5","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["CREATE TABLE flights (\nDEST_COUNTRY_NAME STRING, ORIGIN_COUNTRY_NAME STRING, count LONG)\nUSING JSON OPTIONS (path 'wasbs://mleazuretrainingcontainer@dbrickstraining.blob.core.windows.net/2015-summary.json')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"509184e7-9be3-4772-86cd-d38a7a9e41d0","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### Creating External Tables\n\nHive was one of the first big data SQL\nsystems, and Spark SQL is completely compatible with Hive SQL (HiveQL) statements. One of\nthe use cases that you might encounter is to port your legacy Hive statements to Spark SQL.\n\n\nYou can view any files that have already been defined by running the following command:\n\nCREATE EXTERNAL TABLE hive_flights (\nDEST_COUNTRY_NAME STRING, ORIGIN_COUNTRY_NAME STRING, count LONG)\nROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LOCATION '/data/flight-data-hive/'"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"1799e13c-8ce0-457d-8f3f-f2339f314617","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### Inserting into Tables\nInsertions follow the standard SQL syntax:\n\nINSERT INTO flights_from_select\nSELECT DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME, count FROM flights LIMIT 20"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f87ceceb-9fe3-48d1-a0ef-67d32f4f3b01","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### Describing Table Metadata\nWe saw earlier that you can add a comment when creating a table. You can view this by\ndescribing the table metadata, which will show us the relevant comment:\n    \nDESCRIBE TABLE flights_csv"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"acb79f42-91ce-4120-aa4b-522ab598092e","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### Refreshing Table Metadata\nMaintaining table metadata is an important task to ensure that you’re reading from the most\nrecent set of data. There are two commands to refresh table metadata. REFRESH TABLE refreshes\nall cached entries (essentially, files) associated with the table. If the table were previously\ncached, it would be cached lazily the next time it is scanned:\n    \nREFRESH table partitioned_flights"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"87278b29-93d2-440b-9363-dbb45f98db6c","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### Dropping Tables\nYou cannot delete tables: you can only “drop” them. You can drop a table by using the DROP\nkeyword. If you drop a managed table (e.g., flights_csv), both the data and the table definition\nwill be removed:\n    \nDROP TABLE flights_csv;"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ef974530-46fc-410b-b242-30e7d3b2521c","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### Caching Tables\nJust like DataFrames, you can cache and uncache tables. You simply specify which table you\nwould like using the following syntax:\n    \nCACHE TABLE flights"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c35b10eb-a5f1-49c3-8391-28b8c2f8ae45","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### Views\nNow that you created a table, another thing that you can define is a view. A view specifies a set\nof transformations on top of an existing table—basically just saved query plans, which can be\nconvenient for organizing or reusing your query logic."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5630c948-32ac-4ecb-b62c-ad6a41e9a543","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### Creating Views\nTo an end user, views are displayed as tables, except rather than rewriting all of the data to a new\nlocation, they simply perform a transformation on the source data at query time. This might be a\nfilter, select, or potentially an even larger GROUP BY or ROLLUP. For instance, in the\nfollowing example, we create a view in which the destination is United States in order to see\nonly those flights:\n    \nCREATE VIEW just_usa_view AS\nSELECT * FROM flights WHERE dest_country_name = 'United States'"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"c6b4c6d1-14b4-4df4-8830-a893c2cda2a9","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### Dropping Views\nYou can drop views in the same way that you drop tables; you simply specify that what you\nintend to drop is a view instead of a table. The main difference between dropping a view and\ndropping a table is that with a view, no underlying data is removed, only the view definition\nitself:\n\nDROP VIEW IF EXISTS just_usa_view;"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e134af72-3214-4600-bba3-c3fd116edae9","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### Databases\nDatabases are a tool for organizing tables. As mentioned earlier, if you do not define one, Spark\nwill use the default database."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"b38471ab-f12e-4153-aa83-7acac2e8510f","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### Creating Databases\nCreating databases follows the same patterns you’ve seen previously in this chapter; however,\nhere you use the CREATE DATABASE keywords:\n    \nCREATE DATABASE some_db"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"d55ede26-1624-4c87-816c-6feeb95a8691","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["Setting the Database\nYou might want to set a database to perform a certain query. To do this, use the USE keyword\nfollowed by the database name:\n\nUSE some_db"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"182fd92f-abc4-4211-a8ae-8833388f2ced","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### Dropping Databases\nDropping or removing databases is equally as easy: you simply use the DROP DATABASE\nkeyword:\n    \nDROP DATABASE IF EXISTS some_db;"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"69a0cd08-ec1f-4db4-ac73-826bb41eba02","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### Complex Types\nComplex types are a departure from standard SQL and are an incredibly powerful feature that\ndoes not exist in standard SQL. Understanding how to manipulate them appropriately in SQL is\nessential. There are three core complex types in Spark SQL: structs, lists, and maps."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3451db72-5cfb-49d9-9baf-c62df8fedd20","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["#### Structs\nStructs are more akin to maps. They provide a way of creating or querying nested data in Spark.\nTo create one, you simply need to wrap a set of columns (or expressions) in parentheses:\n\nCREATE VIEW IF NOT EXISTS nested_data AS\nSELECT (DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME) as country, count FROM flights"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a44b0f89-4296-4ad5-98ec-f5b50c969d21","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["#### Lists\nIf you’re familiar with lists in programming languages, Spark SQL lists will feel familiar. There\nare several ways to create an array or list of values. You can use the collect_list function,\nwhich creates a list of values. You can also use the function collect_set, which creates an\narray without duplicate values. These are both aggregation functions and therefore can be\nspecified only in aggregations:\n\nSELECT DEST_COUNTRY_NAME as new_name, collect_list(count) as flight_counts,\ncollect_set(ORIGIN_COUNTRY_NAME) as origin_set\nFROM flights GROUP BY DEST_COUNTRY_NAME"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"045a6a63-9697-42ba-9aae-ac06b1ac8af0","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### Functions\nIn addition to complex types, Spark SQL provides a variety of sophisticated functions. You can\nfind most of these functions in the DataFrames function reference; however, it is worth\nunderstanding how to find these functions in SQL, as well. To see a list of functions in Spark\nSQL, you use the SHOW FUNCTIONS statement:\n    \nSHOW FUNCTIONS"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"894c71ee-3b24-4898-a65a-a33131613348","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### Subqueries\nWith subqueries, you can specify queries within other queries. This makes it possible for you to\nspecify some sophisticated logic within your SQL. In Spark, there are two fundamental\nsubqueries. Correlated subqueries use some information from the outer scope of the query in\norder to supplement information in the subquery. Uncorrelated subqueries include no\ninformation from the outer scope. Each of these queries can return one (scalar subquery) or more\nvalues. Spark also includes support for predicate subqueries, which allow for filtering based on\nvalues."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a5bf8c5a-a196-4f02-9f05-718850e01a87","inputWidgets":{},"title":""}}},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"643f1617-da08-4d72-a8e3-436ea92f5b09","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"mledbrickstraining","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4,"mostRecentlyExecutedCommandWithImplicitDF":{"commandId":3864271172248577,"dataframes":["_sqldf"]}},"language":"python","widgets":{},"notebookOrigID":1410129964961968}},"nbformat":4,"nbformat_minor":0}
